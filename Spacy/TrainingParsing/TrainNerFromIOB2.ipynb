{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def iobToSpacySimple(iobDataset):\n",
    "    \n",
    "    TRAIN_DATA = []\n",
    "    \n",
    "    with open(iobDataset, 'r') as fp:\n",
    "        for line in fp:    \n",
    "            wordTagPairsList = []\n",
    "            \n",
    "            wordTagStringList = line.split(' ')[:-1]\n",
    "            \n",
    "            for wordTagString in wordTagStringList:\n",
    "                wordTagPairList = wordTagString.split('|')\n",
    "                wordTagPairsList.append(wordTagPairList)\n",
    "            \n",
    "            entityStartIndex = 0\n",
    "            entityDict = {}\n",
    "            entityList = []\n",
    "            sentText = ''\n",
    "            currentIndex = 0\n",
    "            \n",
    "            for index, pair in enumerate(wordTagPairsList):\n",
    "                \n",
    "                if pair[1].startswith('B'):\n",
    "                    entityEndIndex = currentIndex + len(pair[0])\n",
    "                    entityStartIndex = currentIndex\n",
    "                    \n",
    "                    for pairInner in wordTagPairsList[index + 1:]:\n",
    "                        if pairInner[1].startswith('O'):\n",
    "                            break\n",
    "                        \n",
    "                        entityEndIndex += (len(pairInner[0]) + 1)\n",
    "                    \n",
    "                    entityTuple = (entityStartIndex, entityEndIndex, pair[1][2:])\n",
    "                    entityList.append(entityTuple) \n",
    "                \n",
    "                sentText += pair[0] + ' '             \n",
    "                currentIndex += len(pair[0]) + 1\n",
    "                \n",
    "            entityDict['entities'] = entityList\n",
    "            trainTuple = (sentText, entityDict)\n",
    "            TRAIN_DATA.append(trainTuple)\n",
    "    \n",
    "    #Remove list elemens in TRAIN_DATA with overlapping entities\n",
    "    overlappingIndexes = []\n",
    "    for index, tup in enumerate(TRAIN_DATA):\n",
    "        endIndex = -1\n",
    "        _, annotation = tup\n",
    "        for ent in annotation.get('entities'):\n",
    "            if (ent[0] < endIndex):\n",
    "                overlappingIndexes.append(index)\n",
    "            endIndex = ent[1]\n",
    "    indexAdjustment = 0        \n",
    "    for ind in overlappingIndexes:\n",
    "        TRAIN_DATA.pop(ind - indexAdjustment)\n",
    "        indexAdjustment += 1\n",
    "    return TRAIN_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def trainNer(TRAIN_DATA, pathToModel, iterations):\n",
    "    nlp = spacy.load(pathToModel)\n",
    "    \n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    \n",
    "    for _, annotations in TRAIN_DATA:\n",
    "         for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "    \n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        nlp.begin_training()\n",
    "        \n",
    "        for itn in range(iterations):\n",
    "            print(\"Statring iteration \" + str(itn))\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "            \n",
    "            print(losses)\n",
    "            \n",
    "    return nlp\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Statring iteration 0\n",
      "{'ner': 40468.645476032034}\n",
      "Statring iteration 1\n",
      "{'ner': 29233.071479270515}\n",
      "Statring iteration 2\n",
      "{'ner': 25581.238843535273}\n",
      "Statring iteration 3\n",
      "{'ner': 23360.7814324469}\n",
      "Statring iteration 4\n",
      "{'ner': 21573.62435180597}\n",
      "Statring iteration 5\n",
      "{'ner': 20378.82527422333}\n",
      "Statring iteration 6\n",
      "{'ner': 19362.70593440347}\n",
      "Statring iteration 7\n",
      "{'ner': 18704.13575881814}\n",
      "Statring iteration 8\n",
      "{'ner': 17825.65488828454}\n",
      "Statring iteration 9\n",
      "{'ner': 17392.407668748587}\n",
      "Statring iteration 10\n",
      "{'ner': 16803.774388647424}\n",
      "Statring iteration 11\n",
      "{'ner': 16289.306068179085}\n",
      "Statring iteration 12\n",
      "{'ner': 16083.101346311776}\n",
      "Statring iteration 13\n",
      "{'ner': 15532.147737091058}\n",
      "Statring iteration 14\n",
      "{'ner': 15351.195557645808}\n",
      "Statring iteration 15\n",
      "{'ner': 14903.363955556664}\n",
      "Statring iteration 16\n",
      "{'ner': 14695.928080697893}\n",
      "Statring iteration 17\n",
      "{'ner': 14425.787008845451}\n",
      "Statring iteration 18\n",
      "{'ner': 14234.783160544283}\n",
      "Statring iteration 19\n",
      "{'ner': 13939.353874403978}\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "TRAIN_DATA = iobToSpacySimple('../Data/DatasetsFormatIOB/SUC30IOB2SelTags')\n",
    "nlp = trainNer(TRAIN_DATA, '../Models/SpacySwedishFastText', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Östervåla\n",
      "Nora\n",
      "Harbo\n",
      "Huddunge\n",
      "år 1887\n",
      "Västmanlands\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "doc = nlp('Våla härad omfattade socknarna Östervåla, Nora, Harbo och Huddunge.Häradsrätten upphörde vid utgången av år 1887 och uppgick i Västmanlands östra domsagas häradsrätt.')\n",
    "for ent in doc.ents:\n",
    "    print(ent)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (RelationExtraction)",
   "language": "python",
   "name": "pycharm-791db5a1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}